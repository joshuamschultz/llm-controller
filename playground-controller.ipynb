{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870b2361",
   "metadata": {},
   "source": [
    "# LLMController Playground\n",
    "\n",
    "This notebook demonstrates how to use the LLMController class with Claude and test various LangChain features.\n",
    "\n",
    "## 1. Setup and Installation\n",
    "\n",
    "Install required packages \n",
    "!pip install langchain anthropic openai python-dotenv langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce66970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, Optional, Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Environment loaded successfully!\")\n",
    "print(f\"Anthropic API Key loaded: {'Yes' if os.getenv('ANTHROPIC_API_KEY') else 'No'}\")\n",
    "print(f\"OpenAI API Key loaded: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47432a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check LangChain version\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"LangChain version: {langchain.__version__}\")\n",
    "except:\n",
    "    print(\"LangChain version not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5cf343",
   "metadata": {},
   "source": [
    "## 2. Import LLMController Class\n",
    "\n",
    "*Note: Make sure you have the LLMController class from the previous artifact saved as `llm_controller.py`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3575f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_controller import LLMController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLMController with Claude\n",
    "llm = LLMController(\n",
    "    llm=\"claude-3-sonnet-20240229\",  # You can change this to claude-3-haiku-20240307 for faster/cheaper\n",
    "    provider=\"claude\"\n",
    ")\n",
    "\n",
    "print(\"LLMController initialized with Claude!\")\n",
    "print(f\"Current model info: {llm.current_model_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic invoke functionality\n",
    "response = llm.invoke(\"Hello! Can you tell me a brief joke about programming?\")\n",
    "print(\"Claude's Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2734f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with message format (like ChatGPT conversation)\n",
    "\n",
    "# Import message classes if not already available\n",
    "try:\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "except ImportError:\n",
    "    from langchain.schema import HumanMessage, AIMessage\n",
    "    \n",
    "messages = [\n",
    "    HumanMessage(content=\"I'm learning Python. Can you explain what a list comprehension is?\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"Claude's explanation of list comprehensions:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ba304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LangChain Prompt Templates\n",
    "try:\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    print(\"✓ Using new prompt imports\")\n",
    "except ImportError:\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.schema import StrOutputParser\n",
    "    print(\"✓ Using legacy prompt imports\")\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful coding tutor. Explain concepts clearly with examples.\"),\n",
    "    (\"human\", \"Explain {concept} in Python with a simple example.\")\n",
    "])\n",
    "\n",
    "# Create a chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test the chain\n",
    "result = chain.invoke({\"concept\": \"decorators\"})\n",
    "print(\"Chain result - Python Decorators:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf17a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming functionality\n",
    "print(\"Streaming response from Claude:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for chunk in llm.stream(\"Write a short poem about artificial intelligence\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test switching between different Claude models\n",
    "models_to_test = [\n",
    "    \"claude-3-haiku-20240307\",    # Fastest, cheapest\n",
    "    \"claude-3-sonnet-20240229\",   # Balanced\n",
    "    \"claude-3-opus-20240229\"      # Most capable (if you have access)\n",
    "]\n",
    "\n",
    "print(\"Testing different Claude models:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model in models_to_test:\n",
    "    try:\n",
    "        llm.switch_model(llm=model, provider=\"claude\")\n",
    "        response = llm.invoke(test_question)\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"Response: {response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{model}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bd553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advanced LangChain Features - Memory\n",
    "\n",
    "try:\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "    print(\"✓ Memory imports successful\")\n",
    "except ImportError:\n",
    "    from langchain.schema import HumanMessage, AIMessage\n",
    "    print(\"✓ Using basic message imports\")\n",
    "    # Memory might not be available, so we'll implement simple history\n",
    "\n",
    "# Create a simple conversation memory\n",
    "conversation_history = []\n",
    "\n",
    "def chat_with_memory(user_input):\n",
    "    \"\"\"Simple chat function with memory\"\"\"\n",
    "    # Add user message\n",
    "    conversation_history.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    # Get response from Claude\n",
    "    response = llm.invoke(conversation_history)\n",
    "    \n",
    "    # Add AI response to history\n",
    "    conversation_history.append(response)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Test conversation with memory\n",
    "print(\"Conversation with memory:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "response1 = chat_with_memory(\"My name is Alice and I love Python programming.\")\n",
    "print(f\"User: My name is Alice and I love Python programming.\")\n",
    "print(f\"Claude: {response1}\")\n",
    "print()\n",
    "\n",
    "response2 = chat_with_memory(\"What's my name and what do I love?\")\n",
    "print(f\"User: What's my name and what do I love?\")\n",
    "print(f\"Claude: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20274d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LangChain Tools (Simple Example)\n",
    "\n",
    "# import libraries\n",
    "try:\n",
    "    # Try new LangChain structure first\n",
    "    from langchain_core.tools import BaseTool\n",
    "    from pydantic import BaseModel, Field\n",
    "    print(\"✓ New tool imports successful\")\n",
    "    TOOLS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Try legacy structure\n",
    "        from langchain.tools import BaseTool\n",
    "        from pydantic import BaseModel, Field\n",
    "        print(\"✓ Legacy tool imports successful\")\n",
    "        TOOLS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Tool imports not available - using function-based approach\")\n",
    "        BaseTool = None\n",
    "        BaseModel = None\n",
    "        Field = None\n",
    "        TOOLS_AVAILABLE = False\n",
    "\n",
    "# Create a simple custom tool\n",
    "if BaseTool and BaseModel:\n",
    "    class CalculatorInput(BaseModel):\n",
    "        expression: str = Field(description=\"Mathematical expression to evaluate\")\n",
    "\n",
    "    class CalculatorTool(BaseTool):\n",
    "        # Properly annotate all fields for Pydantic v2 compatibility\n",
    "        name: str = \"calculator\"\n",
    "        description: str = \"Useful for mathematical calculations\"\n",
    "        args_schema: type[BaseModel] = CalculatorInput\n",
    "        \n",
    "        def _run(self, expression: str) -> str:\n",
    "            try:\n",
    "                # Use a safer eval alternative for production\n",
    "                import ast\n",
    "                import operator\n",
    "                \n",
    "                # Simple math operations mapping\n",
    "                ops = {\n",
    "                    ast.Add: operator.add,\n",
    "                    ast.Sub: operator.sub,\n",
    "                    ast.Mult: operator.mul,\n",
    "                    ast.Div: operator.truediv,\n",
    "                    ast.Pow: operator.pow,\n",
    "                    ast.USub: operator.neg,\n",
    "                }\n",
    "                \n",
    "                def safe_eval(node):\n",
    "                    if isinstance(node, ast.Constant):  # Numbers\n",
    "                        return node.value\n",
    "                    elif isinstance(node, ast.BinOp):  # Binary operations\n",
    "                        return ops[type(node.op)](safe_eval(node.left), safe_eval(node.right))\n",
    "                    elif isinstance(node, ast.UnaryOp):  # Unary operations\n",
    "                        return ops[type(node.op)](safe_eval(node.operand))\n",
    "                    else:\n",
    "                        raise TypeError(f\"Unsupported operation: {type(node)}\")\n",
    "                \n",
    "                try:\n",
    "                    # Parse and evaluate safely\n",
    "                    tree = ast.parse(expression, mode='eval')\n",
    "                    result = safe_eval(tree.body)\n",
    "                    return f\"The result is: {result}\"\n",
    "                except:\n",
    "                    # Fallback to basic eval for simple expressions (be careful in production!)\n",
    "                    result = eval(expression)\n",
    "                    return f\"The result is: {result}\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return f\"Error calculating: {e}\"\n",
    "\n",
    "    # Test the tool\n",
    "    try:\n",
    "        print(\"Testing tool usage with Claude:\")\n",
    "        calculator = CalculatorTool()\n",
    "        calc_result = calculator._run(\"25 * 4 + 10\")\n",
    "        print(f\"Calculator tool result: {calc_result}\")\n",
    "\n",
    "        # Ask Claude to use the result\n",
    "        response = llm.invoke(f\"I calculated 25 * 4 + 10 and got: {calc_result}. Can you verify this is correct?\")\n",
    "        print(f\"Claude's verification: {response.content}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Tool creation failed with error: {e}\")\n",
    "        print(\"This is likely due to LangChain version compatibility issues.\")\n",
    "        print(\"Let's try a simpler approach...\")\n",
    "        \n",
    "        # Simple function-based approach without BaseTool\n",
    "        def simple_calculator(expression: str) -> str:\n",
    "            try:\n",
    "                result = eval(expression)  # Note: Use a safer parser in production\n",
    "                return f\"The result is: {result}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error calculating: {e}\"\n",
    "        \n",
    "        print(\"Using simple function approach:\")\n",
    "        calc_result = simple_calculator(\"25 * 4 + 10\")\n",
    "        print(f\"Calculator result: {calc_result}\")\n",
    "        \n",
    "        response = llm.invoke(f\"I calculated 25 * 4 + 10 and got: {calc_result}. Can you verify this is correct?\")\n",
    "        print(f\"Claude's verification: {response.content}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping tool example due to import issues\")\n",
    "    print(\"This is normal in newer LangChain versions where agent structure has changed\")\n",
    "    \n",
    "    # Alternative: Simple function-based tool demonstration\n",
    "    print(\"\\nUsing simple function approach instead:\")\n",
    "    \n",
    "    def calculator_function(expression: str) -> str:\n",
    "        \"\"\"Simple calculator function\"\"\"\n",
    "        try:\n",
    "            result = eval(expression)  # Note: Use a safer math parser in production\n",
    "            return f\"The result is: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error calculating: {e}\"\n",
    "    \n",
    "    # Test the function\n",
    "    calc_result = calculator_function(\"25 * 4 + 10\")\n",
    "    print(f\"Calculator function result: {calc_result}\")\n",
    "    \n",
    "    # Ask Claude about it\n",
    "    response = llm.invoke(f\"I used a calculator function and got: {calc_result}. Is this calculation correct?\")\n",
    "    print(f\"Claude's response: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Switch to Different Providers (if available)\n",
    "\n",
    "# %%\n",
    "# Test switching to OpenAI if available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"Testing provider switching:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Claude response\n",
    "    llm.switch_model(llm=\"claude-3-sonnet-20240229\", provider=\"claude\")\n",
    "    claude_response = llm.invoke(\"What's the capital of France?\")\n",
    "    print(f\"Claude: {claude_response.content}\")\n",
    "    \n",
    "    # Switch to OpenAI\n",
    "    llm.switch_model(llm=\"gpt-3.5-turbo\", provider=\"openai\")\n",
    "    openai_response = llm.invoke(\"What's the capital of France?\")\n",
    "    print(f\"OpenAI: {openai_response.content}\")\n",
    "    \n",
    "    # Switch back to Claude\n",
    "    llm.switch_model(llm=\"claude-3-sonnet-20240229\", provider=\"claude\")\n",
    "    print(f\"Switched back to: {llm.current_model_info}\")\n",
    "else:\n",
    "    print(\"OpenAI API key not available - skipping provider switching test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35494b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Test - Complex Chain\n",
    "\n",
    "llm = LLMController(\n",
    "    llm=\"claude-3-sonnet-20240229\",  # You can change this to claude-3-haiku-20240307 for faster/cheaper\n",
    "    provider=\"claude\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "except ImportError:\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.schema import StrOutputParser\n",
    "\n",
    "# Create a complex chain for code review\n",
    "code_review_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert Python code reviewer. Provide constructive feedback.\"),\n",
    "    (\"human\", \"\"\"\n",
    "    Please review this Python code and provide feedback:\n",
    "    \n",
    "    ```python\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "    Focus on:\n",
    "    1. Code quality\n",
    "    2. Best practices\n",
    "    3. Potential improvements\n",
    "    4. Any bugs or issues\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "# Sample code to review\n",
    "sample_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for i in range(len(numbers)):\n",
    "        total = total + numbers[i]\n",
    "    return total / len(numbers)\n",
    "\n",
    "result = calculate_average([1, 2, 3, 4, 5])\n",
    "print(result)\n",
    "\"\"\"\n",
    "\n",
    "# Create and run the chain\n",
    "review_chain = code_review_prompt | llm | StrOutputParser()\n",
    "review_result = review_chain.invoke({\"code\": sample_code})\n",
    "\n",
    "print(\"Claude's Code Review:\")\n",
    "print(\"=\" * 50)\n",
    "print(review_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c746821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
