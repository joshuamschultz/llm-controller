{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870b2361",
   "metadata": {},
   "source": [
    "# LLMController Playground\n",
    "\n",
    "This notebook demonstrates how to use the LLMController class with Claude and test various LangChain features.\n",
    "\n",
    "## 1. Setup and Installation\n",
    "\n",
    "Install required packages \n",
    "!pip install langchain anthropic openai python-dotenv langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce66970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, Optional, Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d28b6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded successfully!\n",
      "Anthropic API Key loaded: Yes\n",
      "OpenAI API Key loaded: Yes\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Environment loaded successfully!\")\n",
    "print(f\"Anthropic API Key loaded: {'Yes' if os.getenv('ANTHROPIC_API_KEY') else 'No'}\")\n",
    "print(f\"OpenAI API Key loaded: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47432a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.3.17\n"
     ]
    }
   ],
   "source": [
    "# Check LangChain version\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"LangChain version: {langchain.__version__}\")\n",
    "except:\n",
    "    print(\"LangChain version not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5cf343",
   "metadata": {},
   "source": [
    "## 2. Import LLMController Class\n",
    "\n",
    "*Note: Make sure you have the LLMController class from the previous artifact saved as `llm_controller.py`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3575f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using new LangChain import structure\n",
      "✓ Runnable interface available\n",
      "LLMController class loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from llm_controller import LLMController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4849886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMController initialized with Claude!\n",
      "Current model info: {'provider': 'claude', 'model': 'claude-3-sonnet-20240229', 'type': 'ChatAnthropic'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLMController with Claude\n",
    "llm = LLMController(\n",
    "    llm=\"claude-3-sonnet-20240229\",  # You can change this to claude-3-haiku-20240307 for faster/cheaper\n",
    "    provider=\"claude\"\n",
    ")\n",
    "\n",
    "print(\"LLMController initialized with Claude!\")\n",
    "print(f\"Current model info: {llm.current_model_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da9f620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's Response:\n",
      "Sure, here's a brief programming joke:\n",
      "\n",
      "Why did the programmer quit his job? Because he didn't get arrays.\n"
     ]
    }
   ],
   "source": [
    "# Test basic invoke functionality\n",
    "response = llm.invoke(\"Hello! Can you tell me a brief joke about programming?\")\n",
    "print(\"Claude's Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2734f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's explanation of list comprehensions:\n",
      "Sure, a list comprehension is a concise way to create a new list in Python by applying an expression to each item in an iterable (like a list, tuple, or string).\n",
      "\n",
      "The basic syntax for a list comprehension is:\n",
      "\n",
      "```python\n",
      "new_list = [expression for item in iterable]\n",
      "```\n",
      "\n",
      "Here's a simple example that creates a new list with the squares of numbers from 1 to 10:\n",
      "\n",
      "```python\n",
      "squares = [x**2 for x in range(1, 11)]\n",
      "print(squares)  # Output: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "```\n",
      "\n",
      "You can also add conditions to filter the items in the new list:\n",
      "\n",
      "```python\n",
      "even_squares = [x**2 for x in range(1, 11) if x % 2 == 0]\n",
      "print(even_squares)  # Output: [4, 16, 36, 64, 100]\n",
      "```\n",
      "\n",
      "List comprehensions can be more readable and concise than using a `for` loop and appending to a list, especially for simple operations. However, for more complex operations, a regular `for` loop might be more readable.\n",
      "\n",
      "Here's an example of how you could create the `even_squares` list using a `for` loop:\n",
      "\n",
      "```python\n",
      "even_squares = []\n",
      "for x in range(1, 11):\n",
      "    if x % 2 == 0:\n",
      "        even_squares.append(x**2)\n",
      "print(even_squares)  # Output: [4, 16, 36, 64, 100]\n",
      "```\n",
      "\n",
      "List comprehensions can also be nested to create lists of lists or perform more complex operations.\n"
     ]
    }
   ],
   "source": [
    "# Test with message format (like ChatGPT conversation)\n",
    "\n",
    "# Import message classes if not already available\n",
    "try:\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "except ImportError:\n",
    "    from langchain.schema import HumanMessage, AIMessage\n",
    "    \n",
    "messages = [\n",
    "    HumanMessage(content=\"I'm learning Python. Can you explain what a list comprehension is?\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"Claude's explanation of list comprehensions:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4ba304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using new prompt imports\n",
      "Chain result - Python Decorators:\n",
      "Decorators are a way to modify the behavior of a function without changing its source code. They are a powerful feature in Python that allows you to wrap another function in order to extend its functionality. Decorators are typically used to add logging, caching, authentication, and other cross-cutting concerns to functions.\n",
      "\n",
      "Here's a simple example to illustrate how decorators work:\n",
      "\n",
      "```python\n",
      "def uppercase(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        result = func(*args, **kwargs)\n",
      "        return result.upper()\n",
      "    return wrapper\n",
      "\n",
      "@uppercase\n",
      "def greet(name):\n",
      "    return f\"Hello, {name}!\"\n",
      "\n",
      "print(greet(\"Alice\"))  # Output: HELLO, ALICE!\n",
      "```\n",
      "\n",
      "In this example, we define a decorator function `uppercase` that takes a function `func` as an argument. Inside the `uppercase` function, we define a nested function `wrapper` that takes arbitrary positional and keyword arguments (`*args` and `**kwargs`). The `wrapper` function calls the original function `func` with the provided arguments, and then converts the result to uppercase using the `upper()` method before returning it.\n",
      "\n",
      "The `@uppercase` syntax is a shorthand way of applying the `uppercase` decorator to the `greet` function. It's equivalent to writing `greet = uppercase(greet)`. When we call `greet(\"Alice\")`, the `uppercase` decorator wraps the `greet` function and converts its output to uppercase before returning it.\n",
      "\n",
      "Here's a breakdown of what happens when you call `greet(\"Alice\")`:\n",
      "\n",
      "1. The `greet` function is called with the argument `\"Alice\"`.\n",
      "2. The `uppercase` decorator wraps the `greet` function with the `wrapper` function.\n",
      "3. The `wrapper` function calls the original `greet` function with the argument `\"Alice\"`.\n",
      "4. The `greet` function returns `\"Hello, Alice!\"`.\n",
      "5. The `wrapper` function converts the result to uppercase using `result.upper()`.\n",
      "6. The uppercase string `\"HELLO, ALICE!\"` is returned from the `wrapper` function.\n",
      "\n",
      "Decorators can be stacked by applying multiple decorators to a function. They are executed in the order they are applied, from bottom to top (or inside-out). Decorators can also take arguments, which allows you to parameterize their behavior.\n",
      "\n",
      "Decorators are widely used in Python libraries and frameworks, such as Flask (for routing and view functions), Django (for view functions and model methods), and many others. They provide a clean and elegant way to add functionality to functions without modifying their source code directly.\n"
     ]
    }
   ],
   "source": [
    "# Test LangChain Prompt Templates\n",
    "try:\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    print(\"✓ Using new prompt imports\")\n",
    "except ImportError:\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.schema import StrOutputParser\n",
    "    print(\"✓ Using legacy prompt imports\")\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful coding tutor. Explain concepts clearly with examples.\"),\n",
    "    (\"human\", \"Explain {concept} in Python with a simple example.\")\n",
    "])\n",
    "\n",
    "# Create a chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test the chain\n",
    "result = chain.invoke({\"concept\": \"decorators\"})\n",
    "print(\"Chain result - Python Decorators:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16bf17a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response from Claude:\n",
      "--------------------------------------------------\n",
      "Here's a short poem about artificial intelligence:\n",
      "\n",
      "Artificial Intelligence\n",
      "\n",
      "Circuits ablaze with logic's light,\n",
      "Algorithms dance in binary flight,\n",
      "Machines that learn, adapt, and grow,\n",
      "Unveiling realms we've yet to know.\n",
      "\n",
      "From data streams, they weave their art,\n",
      "Unveiling patterns, playing their part,\n",
      "Augmenting minds with insights new,\n",
      "Transcending boundaries we once knew.\n",
      "\n",
      "A revolution in thought's embrace,\n",
      "Artificial intelligence leads the race,\n",
      "Expanding horizons, defying norms,\n",
      "Reshaping worlds in unconventional forms.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test streaming functionality\n",
    "print(\"Streaming response from Claude:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for chunk in llm.stream(\"Write a short poem about artificial intelligence\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "747c8871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different Claude models:\n",
      "============================================================\n",
      "\n",
      "claude-3-haiku-20240307: Error - name 'test_question' is not defined\n",
      "\n",
      "claude-3-sonnet-20240229: Error - name 'test_question' is not defined\n",
      "\n",
      "claude-3-opus-20240229: Error - name 'test_question' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Test switching between different Claude models\n",
    "models_to_test = [\n",
    "    \"claude-3-haiku-20240307\",    # Fastest, cheapest\n",
    "    \"claude-3-sonnet-20240229\",   # Balanced\n",
    "    \"claude-3-opus-20240229\"      # Most capable (if you have access)\n",
    "]\n",
    "\n",
    "print(\"Testing different Claude models:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model in models_to_test:\n",
    "    try:\n",
    "        llm.switch_model(llm=model, provider=\"claude\")\n",
    "        response = llm.invoke(test_question)\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"Response: {response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{model}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e03bd553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Memory imports successful\n",
      "Conversation with memory:\n",
      "------------------------------\n",
      "User: My name is Alice and I love Python programming.\n",
      "Claude: It's great to meet you, Alice! It's wonderful to hear that you love Python programming. Python is a versatile and beginner-friendly language that is widely used for various applications, including web development, data analysis, artificial intelligence, and more. \n",
      "\n",
      "As an AI language model, I can assist you with any questions or problems you may have related to Python programming. Whether you're just starting out or you're an experienced programmer, feel free to ask me anything, and I'll do my best to provide you with helpful information and guidance.\n",
      "\n",
      "Some topics we could discuss include:\n",
      "\n",
      "1. Basic Python syntax and data types\n",
      "2. Control structures (if/else, loops, etc.)\n",
      "3. Functions and modules\n",
      "4. Object-oriented programming (OOP) in Python\n",
      "5. File handling and data manipulation\n",
      "6. Popular Python libraries and frameworks (e.g., NumPy, Pandas, Django, Flask)\n",
      "7. Best practices and coding style guidelines\n",
      "8. Debugging and troubleshooting techniques\n",
      "\n",
      "Let me know what aspect of Python programming you'd like to explore, and we can dive in together!\n",
      "\n",
      "User: What's my name and what do I love?\n",
      "Claude: Your name is Alice and you love Python programming.\n"
     ]
    }
   ],
   "source": [
    "#Advanced LangChain Features - Memory\n",
    "\n",
    "try:\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "    print(\"✓ Memory imports successful\")\n",
    "except ImportError:\n",
    "    from langchain.schema import HumanMessage, AIMessage\n",
    "    print(\"✓ Using basic message imports\")\n",
    "    # Memory might not be available, so we'll implement simple history\n",
    "\n",
    "# Create a simple conversation memory\n",
    "conversation_history = []\n",
    "\n",
    "def chat_with_memory(user_input):\n",
    "    \"\"\"Simple chat function with memory\"\"\"\n",
    "    # Add user message\n",
    "    conversation_history.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    # Get response from Claude\n",
    "    response = llm.invoke(conversation_history)\n",
    "    \n",
    "    # Add AI response to history\n",
    "    conversation_history.append(response)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Test conversation with memory\n",
    "print(\"Conversation with memory:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "response1 = chat_with_memory(\"My name is Alice and I love Python programming.\")\n",
    "print(f\"User: My name is Alice and I love Python programming.\")\n",
    "print(f\"Claude: {response1}\")\n",
    "print()\n",
    "\n",
    "response2 = chat_with_memory(\"What's my name and what do I love?\")\n",
    "print(f\"User: What's my name and what do I love?\")\n",
    "print(f\"Claude: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20274d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ New tool imports successful\n",
      "Testing tool usage with Claude:\n",
      "Calculator tool result: The result is: 110\n",
      "Claude's verification: I'm sorry, but the result you provided is not correct. Let's solve the problem step by step to find the correct answer.\n",
      "\n",
      "Given:\n",
      "- The problem is to calculate 25 * 4 + 10\n",
      "\n",
      "Step 1: Perform the multiplication first, according to the order of operations (PEMDAS - Parentheses, Exponents, Multiplication and Division from left to right, Addition and Subtraction from left to right).\n",
      "25 * 4 = 100\n",
      "\n",
      "Step 2: Add 10 to the result of the multiplication.\n",
      "100 + 10 = 110\n",
      "\n",
      "Therefore, the correct result of 25 * 4 + 10 is 110.\n"
     ]
    }
   ],
   "source": [
    "# Test LangChain Tools (Simple Example)\n",
    "\n",
    "# import libraries\n",
    "try:\n",
    "    # Try new LangChain structure first\n",
    "    from langchain_core.tools import BaseTool\n",
    "    from pydantic import BaseModel, Field\n",
    "    print(\"✓ New tool imports successful\")\n",
    "    TOOLS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Try legacy structure\n",
    "        from langchain.tools import BaseTool\n",
    "        from pydantic import BaseModel, Field\n",
    "        print(\"✓ Legacy tool imports successful\")\n",
    "        TOOLS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Tool imports not available - using function-based approach\")\n",
    "        BaseTool = None\n",
    "        BaseModel = None\n",
    "        Field = None\n",
    "        TOOLS_AVAILABLE = False\n",
    "\n",
    "# Create a simple custom tool\n",
    "if BaseTool and BaseModel:\n",
    "    class CalculatorInput(BaseModel):\n",
    "        expression: str = Field(description=\"Mathematical expression to evaluate\")\n",
    "\n",
    "    class CalculatorTool(BaseTool):\n",
    "        # Properly annotate all fields for Pydantic v2 compatibility\n",
    "        name: str = \"calculator\"\n",
    "        description: str = \"Useful for mathematical calculations\"\n",
    "        args_schema: type[BaseModel] = CalculatorInput\n",
    "        \n",
    "        def _run(self, expression: str) -> str:\n",
    "            try:\n",
    "                # Use a safer eval alternative for production\n",
    "                import ast\n",
    "                import operator\n",
    "                \n",
    "                # Simple math operations mapping\n",
    "                ops = {\n",
    "                    ast.Add: operator.add,\n",
    "                    ast.Sub: operator.sub,\n",
    "                    ast.Mult: operator.mul,\n",
    "                    ast.Div: operator.truediv,\n",
    "                    ast.Pow: operator.pow,\n",
    "                    ast.USub: operator.neg,\n",
    "                }\n",
    "                \n",
    "                def safe_eval(node):\n",
    "                    if isinstance(node, ast.Constant):  # Numbers\n",
    "                        return node.value\n",
    "                    elif isinstance(node, ast.BinOp):  # Binary operations\n",
    "                        return ops[type(node.op)](safe_eval(node.left), safe_eval(node.right))\n",
    "                    elif isinstance(node, ast.UnaryOp):  # Unary operations\n",
    "                        return ops[type(node.op)](safe_eval(node.operand))\n",
    "                    else:\n",
    "                        raise TypeError(f\"Unsupported operation: {type(node)}\")\n",
    "                \n",
    "                try:\n",
    "                    # Parse and evaluate safely\n",
    "                    tree = ast.parse(expression, mode='eval')\n",
    "                    result = safe_eval(tree.body)\n",
    "                    return f\"The result is: {result}\"\n",
    "                except:\n",
    "                    # Fallback to basic eval for simple expressions (be careful in production!)\n",
    "                    result = eval(expression)\n",
    "                    return f\"The result is: {result}\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return f\"Error calculating: {e}\"\n",
    "\n",
    "    # Test the tool\n",
    "    try:\n",
    "        print(\"Testing tool usage with Claude:\")\n",
    "        calculator = CalculatorTool()\n",
    "        calc_result = calculator._run(\"25 * 4 + 10\")\n",
    "        print(f\"Calculator tool result: {calc_result}\")\n",
    "\n",
    "        # Ask Claude to use the result\n",
    "        response = llm.invoke(f\"I calculated 25 * 4 + 10 and got: {calc_result}. Can you verify this is correct?\")\n",
    "        print(f\"Claude's verification: {response.content}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Tool creation failed with error: {e}\")\n",
    "        print(\"This is likely due to LangChain version compatibility issues.\")\n",
    "        print(\"Let's try a simpler approach...\")\n",
    "        \n",
    "        # Simple function-based approach without BaseTool\n",
    "        def simple_calculator(expression: str) -> str:\n",
    "            try:\n",
    "                result = eval(expression)  # Note: Use a safer parser in production\n",
    "                return f\"The result is: {result}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error calculating: {e}\"\n",
    "        \n",
    "        print(\"Using simple function approach:\")\n",
    "        calc_result = simple_calculator(\"25 * 4 + 10\")\n",
    "        print(f\"Calculator result: {calc_result}\")\n",
    "        \n",
    "        response = llm.invoke(f\"I calculated 25 * 4 + 10 and got: {calc_result}. Can you verify this is correct?\")\n",
    "        print(f\"Claude's verification: {response.content}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping tool example due to import issues\")\n",
    "    print(\"This is normal in newer LangChain versions where agent structure has changed\")\n",
    "    \n",
    "    # Alternative: Simple function-based tool demonstration\n",
    "    print(\"\\nUsing simple function approach instead:\")\n",
    "    \n",
    "    def calculator_function(expression: str) -> str:\n",
    "        \"\"\"Simple calculator function\"\"\"\n",
    "        try:\n",
    "            result = eval(expression)  # Note: Use a safer math parser in production\n",
    "            return f\"The result is: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error calculating: {e}\"\n",
    "    \n",
    "    # Test the function\n",
    "    calc_result = calculator_function(\"25 * 4 + 10\")\n",
    "    print(f\"Calculator function result: {calc_result}\")\n",
    "    \n",
    "    # Ask Claude about it\n",
    "    response = llm.invoke(f\"I used a calculator function and got: {calc_result}. Is this calculation correct?\")\n",
    "    print(f\"Claude's response: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ea403d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing provider switching:\n",
      "------------------------------\n",
      "Claude: The capital of France is Paris.\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: your_ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Switch to OpenAI\u001b[39;00m\n\u001b[32m     15\u001b[39m llm.switch_model(llm=\u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m, provider=\u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m openai_response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms the capital of France?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOpenAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopenai_response.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Switch back to Claude\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/ai-platform/llm_controller.py:117\u001b[39m, in \u001b[36mLLMController.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, config=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Invoke method for LangChain compatibility\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_current_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:995\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    993\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.13/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.13/site-packages/openai/_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your_ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# Test Switch to Different Providers (if available)\n",
    "\n",
    "# %%\n",
    "# Test switching to OpenAI if available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"Testing provider switching:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Claude response\n",
    "    llm.switch_model(llm=\"claude-3-sonnet-20240229\", provider=\"claude\")\n",
    "    claude_response = llm.invoke(\"What's the capital of France?\")\n",
    "    print(f\"Claude: {claude_response.content}\")\n",
    "    \n",
    "    # Switch to OpenAI\n",
    "    llm.switch_model(llm=\"gpt-3.5-turbo\", provider=\"openai\")\n",
    "    openai_response = llm.invoke(\"What's the capital of France?\")\n",
    "    print(f\"OpenAI: {openai_response.content}\")\n",
    "    \n",
    "    # Switch back to Claude\n",
    "    llm.switch_model(llm=\"claude-3-sonnet-20240229\", provider=\"claude\")\n",
    "    print(f\"Switched back to: {llm.current_model_info}\")\n",
    "else:\n",
    "    print(\"OpenAI API key not available - skipping provider switching test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35494b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's Code Review:\n",
      "==================================================\n",
      "The provided Python code appears to be a function that calculates the average of a list of numbers. Here's my feedback based on the requested areas:\n",
      "\n",
      "1. **Code Quality**:\n",
      "   - The code is readable and follows a logical flow.\n",
      "   - The function name `calculate_average` is descriptive and self-explanatory.\n",
      "   - The variable names `total` and `i` are appropriate for their respective purposes.\n",
      "\n",
      "2. **Best Practices**:\n",
      "   - The code follows the Python style guide (PEP 8) for naming conventions and code formatting.\n",
      "   - Using the built-in `sum()` function and `len()` function is a good practice for calculating the total and length of a list, respectively.\n",
      "   - It's a good practice to validate the input data before processing it (e.g., checking if the input is a list, if the list is not empty, and if the list contains only numeric values).\n",
      "\n",
      "3. **Potential Improvements**:\n",
      "   - The code can be simplified by using the built-in `sum()` function and the length of the list, like this:\n",
      "\n",
      "     ```python\n",
      "     def calculate_average(numbers):\n",
      "         if not numbers:\n",
      "             return 0  # or raise an exception, depending on the desired behavior\n",
      "         return sum(numbers) / len(numbers)\n",
      "     ```\n",
      "\n",
      "   - Input validation can be added to handle cases where the input is not a list or contains non-numeric values.\n",
      "   - Instead of printing the result directly, the function could return the calculated average, allowing the caller to decide what to do with the result (e.g., print, store, or perform further operations).\n",
      "\n",
      "4. **Bugs or Issues**:\n",
      "   - The code does not handle the case where an empty list is passed to the `calculate_average` function. This will result in a `ZeroDivisionError` when attempting to divide by zero. It's recommended to handle this case by either returning a default value (e.g., 0) or raising an appropriate exception.\n",
      "   - If the input list contains non-numeric values, the code will raise a `TypeError` when attempting to add them to the `total` variable. Input validation can help prevent this issue.\n",
      "\n",
      "Overall, the code is functional and correctly calculates the average of a list of numbers. However, there are opportunities for improvement in terms of simplification, input validation, and handling edge cases. Implementing the suggested improvements will make the code more robust and maintainable.\n"
     ]
    }
   ],
   "source": [
    "# Final Test - Complex Chain\n",
    "\n",
    "llm = LLMController(\n",
    "    llm=\"claude-3-sonnet-20240229\",  # You can change this to claude-3-haiku-20240307 for faster/cheaper\n",
    "    provider=\"claude\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "except ImportError:\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.schema import StrOutputParser\n",
    "\n",
    "# Create a complex chain for code review\n",
    "code_review_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert Python code reviewer. Provide constructive feedback.\"),\n",
    "    (\"human\", \"\"\"\n",
    "    Please review this Python code and provide feedback:\n",
    "    \n",
    "    ```python\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "    Focus on:\n",
    "    1. Code quality\n",
    "    2. Best practices\n",
    "    3. Potential improvements\n",
    "    4. Any bugs or issues\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "# Sample code to review\n",
    "sample_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for i in range(len(numbers)):\n",
    "        total = total + numbers[i]\n",
    "    return total / len(numbers)\n",
    "\n",
    "result = calculate_average([1, 2, 3, 4, 5])\n",
    "print(result)\n",
    "\"\"\"\n",
    "\n",
    "# Create and run the chain\n",
    "review_chain = code_review_prompt | llm | StrOutputParser()\n",
    "review_result = review_chain.invoke({\"code\": sample_code})\n",
    "\n",
    "print(\"Claude's Code Review:\")\n",
    "print(\"=\" * 50)\n",
    "print(review_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c746821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
